{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctico 2 - Redes en escalera avanzadas\n",
    "\n",
    "Este práctico es similar al práctico 1, pero agregará un paso extra que es el uso de redes en escalera avanzadas, ya sean Redes Convolucionales o Redes Recurrentes.\n",
    "\n",
    "Se les dará, como base, el mismo conjunto de datos de la competencia \"PetFinder\" que se trabajó para el práctico 1, con el agregado de, en este caso, utilizar la descripción como un feature extra y todo el procesamiento que ello requiere.\n",
    "\n",
    "Ahora bien, no es el único conjunto de datos que pueden trabajar. Si tienen un conjunto propio de datos que quieran utilizar y dicho conjunto se preste para el uso de alguna red en escalera avanzada (e.g. conjuntos que tengan imágenes o texto), son libres de hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/dbourdin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dbourdin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn\n",
    "\n",
    "from IPython.display import SVG\n",
    "from gensim import corpora\n",
    "from keras import regularizers\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "nltk.download([\"punkt\", \"stopwords\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'petfinder_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>Description</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>41326</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>41401</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    3     299       0       1       1       7       0             1   \n",
       "1     1    4     307       0       2       1       2       0             2   \n",
       "2     1    1     307       0       1       1       0       0             2   \n",
       "\n",
       "   FurLength  Vaccinated  Dewormed  Sterilized  Health  Quantity  Fee  State  \\\n",
       "0          1           2         2           2       1         1  100  41326   \n",
       "1          1           1         1           2       1         1  150  41401   \n",
       "2          1           2         2           2       1         1    0  41326   \n",
       "\n",
       "                                         Description  AdoptionSpeed  PID  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...              2    0  \n",
       "1  Good guard dog, very alert, active, obedience ...              2    3  \n",
       "2  This handsome yet cute boy is up for adoption....              2    4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv'))\n",
    "\n",
    "target_col = 'AdoptionSpeed'\n",
    "nlabels = dataset[target_col].unique().shape[0]\n",
    "\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproceso del texto para agregarlo como feature (manejo de secuencias)\n",
    "\n",
    "A diferencia del práctico anterior, en este caso es necesario utilizar el texto como feature extra. Pueden luego agregarlo a una red recurrente o convolucional y concatenar su salida a los atributos \"escalares\" (como \"raza\" o \"género\").\n",
    "\n",
    "A continuación les mostraremos los pasos a seguir para ello. La descripción detallada de para que sirve cada paso se encuentra disponible en el [notebook 3](./3_cnns.ipynb).\n",
    "\n",
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SW = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_description(description):\n",
    "    return [w.lower() for w in word_tokenize(description, language=\"english\") if w.lower() not in SW]\n",
    "\n",
    "# Fill the null values with the empty string to avoid errors with NLTK tokenization\n",
    "dataset[\"TokenizedDescription\"] = dataset[\"Description\"].fillna(value=\"\").apply(tokenize_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tamaño de las descripciones\n",
    "\n",
    "Un punto importante a tener en cuenta es que las descripciones tienen tamaño variable, y esto no es compatible con los algoritmos de aprendizaje automático. Por lo que hay que llevar las secuencias a un tamaño uniforme.\n",
    "\n",
    "Para definir dicho tamaño uniforme, es útil mirar qué tamaños mínimos, máximos y medios manejan las descripciones y a partir de esto establecer el tamaño máximo de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10582.000000\n",
      "mean        44.419486\n",
      "std         48.465910\n",
      "min          0.000000\n",
      "25%         16.000000\n",
      "50%         31.000000\n",
      "75%         55.000000\n",
      "max        803.000000\n",
      "Name: TokenizedDescription, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"TokenizedDescription\"].apply(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que más del 75% de las secuencias tienen 55 palabras o menos. Esto es un buen punto de partida, así que podemos establecer el tamaño máximo de las secuencia en 55 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = corpora.Dictionary(dataset[\"TokenizedDescription\"])\n",
    "vocabulary.filter_extremes(no_below=1, no_above=1.0, keep_n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./dataset/glove.6B.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "replace ./dataset/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p dataset\n",
    "# Descargar los word embeddings (GloVe)\n",
    "curl -L -o ./dataset/glove.6B.zip https://cs.famaf.unc.edu.ar/~ccardellino/resources/diplodatos/glove.6B.zip\n",
    "unzip ./dataset/glove.6B.zip glove.6B.100d.txt -d ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7897 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(\"./dataset/glove.6B.100d.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in vocabulary.token2id:  # Only use the embeddings of words in our vocabulary\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found {} word vectors.\".format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los datasets\n",
    "\n",
    "Similar al práctico anterior, tendremos datos que serán \"one-hot-encoded\", otros serán \"embeddings\" y otros serán numéricos.\n",
    "\n",
    "El caso particular del texto es que será tratado como una secuencia de embeddings, y dichos embeddings no serán entrenados en conjunto con la red, sino que serán tomados de un modelo \"pre-entrenado\". En este caso utilizamos GloVe, pero podríamos haber utilizado otro modelo (e.g. FastText)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to always use the same one-hot length\n",
    "one_hot_columns = {\n",
    "    one_hot_col: dataset[one_hot_col].max()\n",
    "    for one_hot_col in ['Type', 'Gender', 'MaturitySize', 'FurLength', \n",
    "                        'Vaccinated', 'Dewormed', 'Sterilized', 'Health']\n",
    "}\n",
    "embedded_columns = {\n",
    "    embedded_col: dataset[embedded_col].max() + 1\n",
    "    for embedded_col in ['Breed1', 'Breed2', 'Color1', 'Color2', 'Color3']\n",
    "}\n",
    "numeric_columns = ['Age', 'Fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador del conjunto de datos\n",
    "\n",
    "Dada la naturaleza de los datos de texto, y que estos representan una secuencia de datos (que se da luego a una red recurrente o convolucional), en este caso no crearemos los datasets de antemano, sino que los generaremos a medida que el algoritmo de entrenamiento los pida. \n",
    "\n",
    "En particular, es porque las secuencias de texto pueden no tener el mismo tamaño (las oraciones tienen diferente cantidad de palabras), pero para que los modelos de redes las acepten, necesitamos rellenarlas (*padding*) de manera que todas tengan el mismo tamaño.\n",
    "\n",
    "En este paso también vamos a truncar aquellas secuencias de descripciones con más de `MAX_SEQUENCE_LEN` palabras, de manera que al hacer uso de `padded_batch` no lance un error al encontrarse con secuencias de tamaño mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las vareiables numéricas\n",
    "dataset[numeric_columns] = sklearn.preprocessing.minmax_scale(dataset[numeric_columns], feature_range=(0, 1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(ds, test_data=False):\n",
    "    for _, row in ds.iterrows():\n",
    "        instance = {}\n",
    "        \n",
    "        direct_features = []\n",
    "        # One hot encoded features\n",
    "        instance[\"direct_features\"] = np.hstack([tf.keras.utils.to_categorical(row[one_hot_col] - 1, max_value)\n",
    "            for one_hot_col, max_value in one_hot_columns.items()])\n",
    "\n",
    "        # Numeric features (should be normalized beforehand)\n",
    "        # TODO: Add numeric features for row\n",
    "        for n_col in numeric_columns:\n",
    "            instance[n_col] = [row[n_col]]\n",
    "        \n",
    "        # Embedded features\n",
    "        for embedded_col in embedded_columns:\n",
    "            instance[embedded_col] = [row[embedded_col]]\n",
    "        \n",
    "        # Document to indices for text data, truncated at MAX_SEQUENCE_LEN words\n",
    "        instance[\"description\"] = vocabulary.doc2idx(\n",
    "            row[\"TokenizedDescription\"],\n",
    "            unknown_word_index=len(vocabulary)\n",
    "        )[:MAX_SEQUENCE_LEN]\n",
    "        \n",
    "        # One hot encoded target for categorical crossentropy\n",
    "        if not test_data:\n",
    "            target = tf.keras.utils.to_categorical(row[target_col], nlabels)\n",
    "            yield instance, target\n",
    "        else:\n",
    "            yield instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': <tf.Tensor: id=49, shape=(1,), dtype=float32, numpy=array([0.01176471], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=50, shape=(1,), dtype=int32, numpy=array([299], dtype=int32)>,\n",
      " 'Breed2': <tf.Tensor: id=51, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Color1': <tf.Tensor: id=52, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'Color2': <tf.Tensor: id=53, shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>,\n",
      " 'Color3': <tf.Tensor: id=54, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=55, shape=(1,), dtype=float32, numpy=array([0.03333334], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=56, shape=(42,), dtype=int32, numpy=\n",
      "array([23,  2, 20, 24,  4, 10,  1, 11, 26,  1, 27,  9,  6, 21,  3,  8, 15,\n",
      "       22, 33,  7, 13, 30,  1, 29, 18, 17,  1, 12, 31, 14,  5,  6, 16,  1,\n",
      "       19, 28, 25, 32, 23,  0,  5,  1], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=57, shape=(24,), dtype=float32, numpy=\n",
      "array([0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "       0., 0., 1., 0., 1., 0., 0.], dtype=float32)>}\n",
      "<tf.Tensor: id=58, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
      "\n",
      "{'Age': <tf.Tensor: id=59, shape=(1,), dtype=float32, numpy=array([0.01568628], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=60, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
      " 'Breed2': <tf.Tensor: id=61, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Color1': <tf.Tensor: id=62, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'Color2': <tf.Tensor: id=63, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n",
      " 'Color3': <tf.Tensor: id=64, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=65, shape=(1,), dtype=float32, numpy=array([0.05], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=66, shape=(24,), dtype=int32, numpy=\n",
      "array([41, 42, 40, 35, 37, 35, 36, 35, 45, 50, 41, 44, 35, 46, 38, 48, 39,\n",
      "       47, 15, 43, 35, 49, 34, 34], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=67, shape=(24,), dtype=float32, numpy=\n",
      "array([1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0.], dtype=float32)>}\n",
      "<tf.Tensor: id=68, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set output types of the generator (for numeric types check the type is valid)\n",
    "instance_types = {\n",
    "    \"direct_features\": tf.float32,\n",
    "    \"description\": tf.int32\n",
    "}\n",
    "\n",
    "for embedded_col in embedded_columns:\n",
    "    instance_types[embedded_col] = tf.int32\n",
    "\n",
    "for n_col in numeric_columns:\n",
    "    instance_types[n_col] = tf.float32\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_generator(dataset),\n",
    "    output_types=(instance_types, tf.int32)\n",
    ")\n",
    "\n",
    "for data, target in tf_dataset.take(2):\n",
    "    pprint(data)\n",
    "    pprint(target)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrenamiento y validación\n",
    "\n",
    "Ya generado el conjunto de datos base, tenemos que dividirlo en entrenamiento y validación. Además, como vamos a utilizar algunos datos que forman secuencias, los lotes (*batches*) de datos deben estar \"rellenados\" (*padded_batch*). \n",
    "\n",
    "Si bien rellenaremos \"todos\" los atributos, en la práctica el único que efectivamente se rellenará es el de *description* pues es el único con tamaños distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(dataset.shape[0] * 0.8)\n",
    "DEV_SIZE = dataset.shape[0] - TRAIN_SIZE\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "shuffled_dataset = tf_dataset.shuffle(TRAIN_SIZE + DEV_SIZE, seed=42)\n",
    "\n",
    "# Pad the datasets to the max value for all the \"non sequence\" features\n",
    "padding_shapes = (\n",
    "    {k: [-1] for k in [\"direct_features\"] + list(embedded_columns.keys()) + list([(numeric_columns)[0]]) + list([(numeric_columns)[1]])},\n",
    "    [-1]\n",
    ")\n",
    "\n",
    "# Pad to MAX_SEQUENCE_LEN for sequence features\n",
    "padding_shapes[0][\"description\"] = [MAX_SEQUENCE_LEN]\n",
    "\n",
    "# Pad values are irrelevant for non padded data\n",
    "padding_values = (\n",
    "    {k: 0 for k in list(embedded_columns.keys())},\n",
    "    0\n",
    ")\n",
    "\n",
    "# Padding value for direct features should be a float\n",
    "padding_values[0][\"direct_features\"] = np.float32(0)\n",
    "\n",
    "# Padding value for sequential features is the vocabulary length + 1\n",
    "padding_values[0][\"description\"] = len(vocabulary) + 1\n",
    "\n",
    "# Agregamos para las columnas numéricas\n",
    "padding_values[0][numeric_columns[0]] = np.float32(0)\n",
    "padding_values[0][numeric_columns[1]] = np.float32(0)\n",
    "\n",
    "\n",
    "train_dataset = shuffled_dataset.skip(DEV_SIZE)\\\n",
    "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)\n",
    "\n",
    "dev_dataset = shuffled_dataset.take(DEV_SIZE)\\\n",
    "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el modelo\n",
    "\n",
    "Al modelo anterior tenemos que agregarle la capa que maneje los embeddings de las palabras, e inicializarla de manera acorde, podemos guiarnos por lo visto en el [notebook 3](./3_cnns.ipynb) para hacer esto.\n",
    "\n",
    "### Matriz de embeddings de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 100  # Given by the model (in this case glove.6B.100d)\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocabulary) + 2, 100))\n",
    "\n",
    "for widx, word in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[widx] = embedding_vector\n",
    "    else:\n",
    "        # Random normal initialization for words without embeddings\n",
    "        embedding_matrix[widx] = np.random.normal(size=(100,))  \n",
    "\n",
    "# Random normal initialization for unknown words\n",
    "embedding_matrix[len(vocabulary)] = np.random.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo los inputs del modelo\n",
    "\n",
    "Definamos los inputs del modelo, con el agregado de la capa de embeddings de palabras inicializada en `embedding_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding embedding of size 77 for layer Breed1\n",
      "Adding embedding of size 77 for layer Breed2\n",
      "Adding embedding of size 2 for layer Color1\n",
      "Adding embedding of size 2 for layer Color2\n",
      "Adding embedding of size 2 for layer Color3\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Add one input and one embedding for each embedded column\n",
    "embedding_layers = []\n",
    "numeric_layer = []\n",
    "inputs = []\n",
    "for embedded_col, max_value in embedded_columns.items():\n",
    "    input_layer = tf.keras.layers.Input(shape=(1,), name=embedded_col)\n",
    "    inputs.append(input_layer)\n",
    "    # Define the embedding layer\n",
    "    embedding_size = int(max_value / 4)\n",
    "    embedding_layers.append(\n",
    "        tf.squeeze(\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=max_value, \n",
    "                output_dim=embedding_size\n",
    "            )(input_layer), \n",
    "            axis=-2\n",
    "        )\n",
    "    )\n",
    "    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "# Add the direct features already calculated\n",
    "direct_features_input = tf.keras.layers.Input(\n",
    "    shape=(sum(one_hot_columns.values()),), \n",
    "    name='direct_features'\n",
    ")\n",
    "inputs.append(direct_features_input)\n",
    "\n",
    "# Add numeric columns\n",
    "# Add the direct features already calculated\n",
    "numerical_inputs = [tf.keras.layers.Input(shape=(1,), name=numeric_col) for numeric_col in numeric_columns]\n",
    "inputs += numerical_inputs\n",
    "# numerical_input = tf.keras.layers.Input(\n",
    "#     shape=(1,), \n",
    "#     name='Age'\n",
    "# )\n",
    "# inputs.append(numerical_input)\n",
    "\n",
    "\n",
    "# numerical_input1 = tf.keras.layers.Input(\n",
    "#     shape=(1,), \n",
    "#     name='Fee'\n",
    "# )\n",
    "# inputs.append(numerical_input1)\n",
    "\n",
    "# Word embedding layer\n",
    "description_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), name=\"description\")\n",
    "inputs.append(description_input)\n",
    "\n",
    "word_embeddings_layer = tf.keras.layers.Embedding(\n",
    "    embedding_matrix.shape[0],\n",
    "    EMBEDDINGS_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LEN,\n",
    "    trainable=False,\n",
    "    name=\"word_embedding\"\n",
    ")(description_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo la red que trabajará con el texto\n",
    "\n",
    "Antes de generar el *feature map* final entre los inputs y las clases, tenemos que generar el *feature map* de las secuencias de texto. \n",
    "\n",
    "Para ello pueden utilizar una red neuronal recurrente o convolucional.\n",
    "\n",
    "Pueden pensar dicha red como un submodelo del modelo general que se encarga de generar los atributos que representan la descripción de la mascota (recordemos que las redes se utilizan para hacer aprendizaje de representaciones).\n",
    "\n",
    "La red puede ser tan compleja como ustedes lo consideren pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_WIDTHS = [2, 3, 5]  # Take 2, 3, and 5 words\n",
    "FILTER_COUNT = 64\n",
    "\n",
    "conv_layers = []\n",
    "for filter_width in FILTER_WIDTHS:\n",
    "    layer = tf.keras.layers.Conv1D(\n",
    "        FILTER_COUNT,\n",
    "        filter_width,\n",
    "        activation=\"relu\",\n",
    "        name=\"conv_{}_words\".format(filter_width)\n",
    "    )(word_embeddings_layer)\n",
    "    layer = tf.keras.layers.GlobalMaxPooling1D(name=\"max_pool_{}_words\".format(filter_width))(layer)\n",
    "    conv_layers.append(layer)\n",
    "\n",
    "description_features = tf.keras.layers.Concatenate(name=\"description_features\")(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Create a NN (CNN or RNN) for the description input (replace the next)\n",
    "# DESCRIPTION_FEATURES_LAYER_SIZE = 512\n",
    "\n",
    "# description_features = tf.keras.layers.SimpleRNN(256)(word_embeddings_layer)\n",
    "# description_features = tf.keras.layers.Flatten()(description_features)  # This is a simple concatenation\n",
    "# description_features = tf.keras.layers.Dense(\n",
    "#     units=DESCRIPTION_FEATURES_LAYER_SIZE, \n",
    "#     activation=\"relu\", \n",
    "#     name=\"description_features\")(description_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo el *feature map* final de la red\n",
    "\n",
    "Ahora que tenemos nuestra representación de las descripciones, pasamos a combinarlo con los demás features en la última parte de nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "feature_map = tf.keras.layers.Concatenate(name=\"feature_map\")(\n",
    "    embedding_layers + numerical_inputs +  [description_features, direct_features_input]\n",
    ")\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation=\"relu\",\n",
    "                                     kernel_regularizer=regularizers.l2(0.001))(feature_map)\n",
    "hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dropout(0.3)(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE/2, activation=\"relu\",\n",
    "                                     kernel_regularizer=regularizers.l2(0.001))(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dropout(0.3)(hidden_layer)\n",
    "output_layer = tf.keras.layers.Dense(nlabels, activation=\"softmax\", name=\"output\")(hidden_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=[output_layer], name=\"amazing_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilando y visualizando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amazing_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "description (InputLayer)        [(None, 55)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 55, 100)      1000200     description[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Breed2 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color2 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color3 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_words (Conv1D)           (None, 54, 64)       12864       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_3_words (Conv1D)           (None, 53, 64)       19264       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_5_words (Conv1D)           (None, 51, 64)       32064       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 77)        23716       Breed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 2)         16          Color1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 2)         16          Color2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 2)         16          Color3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_2_words (GlobalMaxPool (None, 64)           0           conv_2_words[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_3_words (GlobalMaxPool (None, 64)           0           conv_3_words[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_5_words (GlobalMaxPool (None, 64)           0           conv_5_words[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, 77)]         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_2 (TensorFl [(None, 2)]          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_3 (TensorFl [(None, 2)]          0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_4 (TensorFl [(None, 2)]          0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Fee (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "description_features (Concatena (None, 192)          0           max_pool_2_words[0][0]           \n",
      "                                                                 max_pool_3_words[0][0]           \n",
      "                                                                 max_pool_5_words[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "direct_features (InputLayer)    [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_map (Concatenate)       (None, 378)          0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 tf_op_layer_Squeeze_1[0][0]      \n",
      "                                                                 tf_op_layer_Squeeze_2[0][0]      \n",
      "                                                                 tf_op_layer_Squeeze_3[0][0]      \n",
      "                                                                 tf_op_layer_Squeeze_4[0][0]      \n",
      "                                                                 Age[0][0]                        \n",
      "                                                                 Fee[0][0]                        \n",
      "                                                                 description_features[0][0]       \n",
      "                                                                 direct_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          48512       feature_map[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 5)            325         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,169,733\n",
      "Trainable params: 169,149\n",
      "Non-trainable params: 1,000,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"767pt\" viewBox=\"0.00 0.00 2530.50 921.00\" width=\"2109pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 917)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-917 2526.5,-917 2526.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139793401169232 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139793401169232</title>\n",
       "<polygon fill=\"none\" points=\"325.5,-876.5 325.5,-912.5 469.5,-912.5 469.5,-876.5 325.5,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-890.8\">description: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793401088880 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139793401088880</title>\n",
       "<polygon fill=\"none\" points=\"306.5,-803.5 306.5,-839.5 488.5,-839.5 488.5,-803.5 306.5,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-817.8\">word_embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139793401169232&#45;&gt;139793401088880 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139793401169232-&gt;139793401088880</title>\n",
       "<path d=\"M397.5,-876.4551C397.5,-868.3828 397.5,-858.6764 397.5,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-849.5903 397.5,-839.5904 394.0001,-849.5904 401.0001,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402333056 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139793402333056</title>\n",
       "<polygon fill=\"none\" points=\"104,-730.5 104,-766.5 253,-766.5 253,-730.5 104,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-744.8\">conv_2_words: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139793401088880&#45;&gt;139793402333056 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139793401088880-&gt;139793402333056</title>\n",
       "<path d=\"M343.3652,-803.4551C312.8311,-793.277 274.5028,-780.5009 242.4883,-769.8294\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"243.3647,-766.4323 232.7711,-766.5904 241.1511,-773.0731 243.3647,-766.4323\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793470247544 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>139793470247544</title>\n",
       "<polygon fill=\"none\" points=\"323,-730.5 323,-766.5 472,-766.5 472,-730.5 323,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-744.8\">conv_3_words: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139793401088880&#45;&gt;139793470247544 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139793401088880-&gt;139793470247544</title>\n",
       "<path d=\"M397.5,-803.4551C397.5,-795.3828 397.5,-785.6764 397.5,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-776.5903 397.5,-766.5904 394.0001,-776.5904 401.0001,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401589096 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>139793401589096</title>\n",
       "<polygon fill=\"none\" points=\"542,-730.5 542,-766.5 691,-766.5 691,-730.5 542,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616.5\" y=\"-744.8\">conv_5_words: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139793401088880&#45;&gt;139793401589096 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139793401088880-&gt;139793401589096</title>\n",
       "<path d=\"M451.6348,-803.4551C482.1689,-793.277 520.4972,-780.5009 552.5117,-769.8294\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"553.8489,-773.0731 562.2289,-766.5904 551.6353,-766.4323 553.8489,-773.0731\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793542113152 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139793542113152</title>\n",
       "<polygon fill=\"none\" points=\"826,-730.5 826,-766.5 949,-766.5 949,-730.5 826,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"887.5\" y=\"-744.8\">Breed1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793470179200 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>139793470179200</title>\n",
       "<polygon fill=\"none\" points=\"813.5,-657.5 813.5,-693.5 961.5,-693.5 961.5,-657.5 813.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"887.5\" y=\"-671.8\">embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139793542113152&#45;&gt;139793470179200 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139793542113152-&gt;139793470179200</title>\n",
       "<path d=\"M887.5,-730.4551C887.5,-722.3828 887.5,-712.6764 887.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"891.0001,-703.5903 887.5,-693.5904 884.0001,-703.5904 891.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402225664 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139793402225664</title>\n",
       "<polygon fill=\"none\" points=\"1050,-730.5 1050,-766.5 1173,-766.5 1173,-730.5 1050,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-744.8\">Breed2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793402225608 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>139793402225608</title>\n",
       "<polygon fill=\"none\" points=\"1031,-657.5 1031,-693.5 1192,-693.5 1192,-657.5 1031,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-671.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 139793402225664&#45;&gt;139793402225608 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139793402225664-&gt;139793402225608</title>\n",
       "<path d=\"M1111.5,-730.4551C1111.5,-722.3828 1111.5,-712.6764 1111.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1115.0001,-703.5903 1111.5,-693.5904 1108.0001,-703.5904 1115.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792872265152 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139792872265152</title>\n",
       "<polygon fill=\"none\" points=\"1333.5,-730.5 1333.5,-766.5 1455.5,-766.5 1455.5,-730.5 1333.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1394.5\" y=\"-744.8\">Color1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139792872264592 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>139792872264592</title>\n",
       "<polygon fill=\"none\" points=\"1314,-657.5 1314,-693.5 1475,-693.5 1475,-657.5 1314,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1394.5\" y=\"-671.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 139792872265152&#45;&gt;139792872264592 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139792872265152-&gt;139792872264592</title>\n",
       "<path d=\"M1394.5,-730.4551C1394.5,-722.3828 1394.5,-712.6764 1394.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1398.0001,-703.5903 1394.5,-693.5904 1391.0001,-703.5904 1398.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401292392 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139793401292392</title>\n",
       "<polygon fill=\"none\" points=\"1616.5,-730.5 1616.5,-766.5 1738.5,-766.5 1738.5,-730.5 1616.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-744.8\">Color2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793401291832 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>139793401291832</title>\n",
       "<polygon fill=\"none\" points=\"1597,-657.5 1597,-693.5 1758,-693.5 1758,-657.5 1597,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-671.8\">embedding_3: Embedding</text>\n",
       "</g>\n",
       "<!-- 139793401292392&#45;&gt;139793401291832 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>139793401292392-&gt;139793401291832</title>\n",
       "<path d=\"M1677.5,-730.4551C1677.5,-722.3828 1677.5,-712.6764 1677.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-703.5903 1677.5,-693.5904 1674.0001,-703.5904 1681.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401239312 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139793401239312</title>\n",
       "<polygon fill=\"none\" points=\"1899.5,-730.5 1899.5,-766.5 2021.5,-766.5 2021.5,-730.5 1899.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1960.5\" y=\"-744.8\">Color3: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793401238752 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>139793401238752</title>\n",
       "<polygon fill=\"none\" points=\"1880,-657.5 1880,-693.5 2041,-693.5 2041,-657.5 1880,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1960.5\" y=\"-671.8\">embedding_4: Embedding</text>\n",
       "</g>\n",
       "<!-- 139793401239312&#45;&gt;139793401238752 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>139793401239312-&gt;139793401238752</title>\n",
       "<path d=\"M1960.5,-730.4551C1960.5,-722.3828 1960.5,-712.6764 1960.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1964.0001,-703.5903 1960.5,-693.5904 1957.0001,-703.5904 1964.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402224712 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>139793402224712</title>\n",
       "<polygon fill=\"none\" points=\"0,-657.5 0,-693.5 253,-693.5 253,-657.5 0,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126.5\" y=\"-671.8\">max_pool_2_words: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 139793402333056&#45;&gt;139793402224712 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>139793402333056-&gt;139793402224712</title>\n",
       "<path d=\"M165.6461,-730.4551C159.521,-721.8564 152.075,-711.4034 145.328,-701.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"148.0389,-699.7046 139.3863,-693.5904 142.3374,-703.7659 148.0389,-699.7046\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402320264 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>139793402320264</title>\n",
       "<polygon fill=\"none\" points=\"271,-657.5 271,-693.5 524,-693.5 524,-657.5 271,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-671.8\">max_pool_3_words: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 139793470247544&#45;&gt;139793402320264 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>139793470247544-&gt;139793402320264</title>\n",
       "<path d=\"M397.5,-730.4551C397.5,-722.3828 397.5,-712.6764 397.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-703.5903 397.5,-693.5904 394.0001,-703.5904 401.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793470180432 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>139793470180432</title>\n",
       "<polygon fill=\"none\" points=\"542,-657.5 542,-693.5 795,-693.5 795,-657.5 542,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668.5\" y=\"-671.8\">max_pool_5_words: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 139793401589096&#45;&gt;139793470180432 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>139793401589096-&gt;139793470180432</title>\n",
       "<path d=\"M629.3539,-730.4551C635.479,-721.8564 642.925,-711.4034 649.672,-701.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"652.6626,-703.7659 655.6137,-693.5904 646.9611,-699.7046 652.6626,-703.7659\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401369488 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>139793401369488</title>\n",
       "<polygon fill=\"none\" points=\"710,-584.5 710,-620.5 961,-620.5 961,-584.5 710,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"835.5\" y=\"-598.8\">tf_op_layer_Squeeze: TensorFlowOpLayer</text>\n",
       "</g>\n",
       "<!-- 139793470179200&#45;&gt;139793401369488 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>139793470179200-&gt;139793401369488</title>\n",
       "<path d=\"M874.6461,-657.4551C868.521,-648.8564 861.075,-638.4034 854.328,-628.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"857.0389,-626.7046 848.3863,-620.5904 851.3374,-630.7659 857.0389,-626.7046\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792872101312 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>139792872101312</title>\n",
       "<polygon fill=\"none\" points=\"979,-584.5 979,-620.5 1244,-620.5 1244,-584.5 979,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-598.8\">tf_op_layer_Squeeze_1: TensorFlowOpLayer</text>\n",
       "</g>\n",
       "<!-- 139793402225608&#45;&gt;139792872101312 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>139793402225608-&gt;139792872101312</title>\n",
       "<path d=\"M1111.5,-657.4551C1111.5,-649.3828 1111.5,-639.6764 1111.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1115.0001,-630.5903 1111.5,-620.5904 1108.0001,-630.5904 1115.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793506147912 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>139793506147912</title>\n",
       "<polygon fill=\"none\" points=\"1262,-584.5 1262,-620.5 1527,-620.5 1527,-584.5 1262,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1394.5\" y=\"-598.8\">tf_op_layer_Squeeze_2: TensorFlowOpLayer</text>\n",
       "</g>\n",
       "<!-- 139792872264592&#45;&gt;139793506147912 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>139792872264592-&gt;139793506147912</title>\n",
       "<path d=\"M1394.5,-657.4551C1394.5,-649.3828 1394.5,-639.6764 1394.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1398.0001,-630.5903 1394.5,-620.5904 1391.0001,-630.5904 1398.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401370496 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>139793401370496</title>\n",
       "<polygon fill=\"none\" points=\"1545,-584.5 1545,-620.5 1810,-620.5 1810,-584.5 1545,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-598.8\">tf_op_layer_Squeeze_3: TensorFlowOpLayer</text>\n",
       "</g>\n",
       "<!-- 139793401291832&#45;&gt;139793401370496 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>139793401291832-&gt;139793401370496</title>\n",
       "<path d=\"M1677.5,-657.4551C1677.5,-649.3828 1677.5,-639.6764 1677.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-630.5903 1677.5,-620.5904 1674.0001,-630.5904 1681.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401371112 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>139793401371112</title>\n",
       "<polygon fill=\"none\" points=\"1828,-584.5 1828,-620.5 2093,-620.5 2093,-584.5 1828,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1960.5\" y=\"-598.8\">tf_op_layer_Squeeze_4: TensorFlowOpLayer</text>\n",
       "</g>\n",
       "<!-- 139793401238752&#45;&gt;139793401371112 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>139793401238752-&gt;139793401371112</title>\n",
       "<path d=\"M1960.5,-657.4551C1960.5,-649.3828 1960.5,-639.6764 1960.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1964.0001,-630.5903 1960.5,-620.5904 1957.0001,-630.5904 1964.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402333224 -->\n",
       "<g class=\"node\" id=\"node26\">\n",
       "<title>139793402333224</title>\n",
       "<polygon fill=\"none\" points=\"394.5,-584.5 394.5,-620.5 594.5,-620.5 594.5,-584.5 394.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"494.5\" y=\"-598.8\">description_features: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139793402224712&#45;&gt;139793402333224 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>139793402224712-&gt;139793402333224</title>\n",
       "<path d=\"M217.4663,-657.4551C270.8652,-646.8623 338.4505,-633.4555 393.4977,-622.5358\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"394.4963,-625.906 403.6241,-620.527 393.1342,-619.0398 394.4963,-625.906\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402320264&#45;&gt;139793402333224 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>139793402320264-&gt;139793402333224</title>\n",
       "<path d=\"M421.4775,-657.4551C433.8359,-648.1545 449.0769,-636.6844 462.4326,-626.6332\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"464.5766,-629.4001 470.4621,-620.5904 460.3674,-623.807 464.5766,-629.4001\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793470180432&#45;&gt;139793402333224 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>139793470180432-&gt;139793402333224</title>\n",
       "<path d=\"M625.4888,-657.4551C601.7517,-647.4964 572.0859,-635.0504 546.9915,-624.5223\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"548.1949,-621.2317 537.6195,-620.5904 545.4868,-627.6866 548.1949,-621.2317\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401368928 -->\n",
       "<g class=\"node\" id=\"node28\">\n",
       "<title>139793401368928</title>\n",
       "<polygon fill=\"none\" points=\"1598,-511.5 1598,-547.5 1757,-547.5 1757,-511.5 1598,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-525.8\">feature_map: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139793401369488&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>139793401369488-&gt;139793401368928</title>\n",
       "<path d=\"M961.2253,-585.0224C964.3466,-584.6687 967.4421,-584.3271 970.5,-584 1191.8031,-560.328 1453.1378,-542.9098 1587.5912,-534.7185\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1588.0685,-538.1961 1597.8383,-534.0971 1587.6447,-531.2089 1588.0685,-538.1961\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792872101312&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>139792872101312-&gt;139793401368928</title>\n",
       "<path d=\"M1244.1524,-585.2105C1247.2985,-584.8028 1250.4181,-584.3988 1253.5,-584 1368.9036,-569.0657 1501.995,-551.981 1587.8178,-540.9822\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1588.3116,-544.4476 1597.7856,-539.7049 1587.4219,-537.5044 1588.3116,-544.4476\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793506147912&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>139793506147912-&gt;139793401368928</title>\n",
       "<path d=\"M1464.4551,-584.4551C1504.7627,-574.0577 1555.5812,-560.949 1597.4752,-550.1424\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1598.5601,-553.4772 1607.3689,-547.5904 1596.8116,-546.6991 1598.5601,-553.4772\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401370496&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>139793401370496-&gt;139793401368928</title>\n",
       "<path d=\"M1677.5,-584.4551C1677.5,-576.3828 1677.5,-566.6764 1677.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-557.5903 1677.5,-547.5904 1674.0001,-557.5904 1681.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401371112&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>139793401371112-&gt;139793401368928</title>\n",
       "<path d=\"M1890.5449,-584.4551C1850.2373,-574.0577 1799.4188,-560.949 1757.5248,-550.1424\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1758.1884,-546.6991 1747.6311,-547.5904 1756.4399,-553.4772 1758.1884,-546.6991\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401169680 -->\n",
       "<g class=\"node\" id=\"node24\">\n",
       "<title>139793401169680</title>\n",
       "<polygon fill=\"none\" points=\"2111,-584.5 2111,-620.5 2218,-620.5 2218,-584.5 2111,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2164.5\" y=\"-598.8\">Age: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793401169680&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge26\">\n",
       "<title>139793401169680-&gt;139793401368928</title>\n",
       "<path d=\"M2110.869,-585.9164C2108.0492,-585.2267 2105.2472,-584.582 2102.5,-584 1988.0464,-559.7537 1853.8167,-544.7291 1767.307,-536.7467\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1767.5362,-533.2532 1757.2599,-535.8316 1766.9011,-540.2244 1767.5362,-533.2532\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401087200 -->\n",
       "<g class=\"node\" id=\"node25\">\n",
       "<title>139793401087200</title>\n",
       "<polygon fill=\"none\" points=\"2236.5,-584.5 2236.5,-620.5 2340.5,-620.5 2340.5,-584.5 2236.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2288.5\" y=\"-598.8\">Fee: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793401087200&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge27\">\n",
       "<title>139793401087200-&gt;139793401368928</title>\n",
       "<path d=\"M2236.4271,-585.9752C2233.4212,-585.2501 2230.4304,-584.5837 2227.5,-584 2067.3454,-552.099 1877.0578,-538.5269 1767.4818,-533.0124\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1767.6188,-529.515 1757.4588,-532.5192 1767.2747,-536.5066 1767.6188,-529.515\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793402333224&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge28\">\n",
       "<title>139793402333224-&gt;139793401368928</title>\n",
       "<path d=\"M594.7308,-592.8637C628.6174,-589.7865 666.6835,-586.5272 701.5,-584 1027.6059,-560.3294 1415.659,-541.4191 1587.7163,-533.5107\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1587.91,-537.0056 1597.7392,-533.0512 1587.5894,-530.0129 1587.91,-537.0056\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793506373472 -->\n",
       "<g class=\"node\" id=\"node27\">\n",
       "<title>139793506373472</title>\n",
       "<polygon fill=\"none\" points=\"2358.5,-584.5 2358.5,-620.5 2522.5,-620.5 2522.5,-584.5 2358.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2440.5\" y=\"-598.8\">direct_features: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139793506373472&#45;&gt;139793401368928 -->\n",
       "<g class=\"edge\" id=\"edge29\">\n",
       "<title>139793506373472-&gt;139793401368928</title>\n",
       "<path d=\"M2358.2366,-585.3171C2355.2923,-584.8484 2352.3732,-584.4071 2349.5,-584 2142.3038,-554.6443 1896.7573,-539.7101 1767.4806,-533.3717\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1767.3943,-529.8635 1757.2366,-532.8755 1767.0555,-536.8553 1767.3943,-529.8635\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401372176 -->\n",
       "<g class=\"node\" id=\"node29\">\n",
       "<title>139793401372176</title>\n",
       "<polygon fill=\"none\" points=\"1633,-438.5 1633,-474.5 1722,-474.5 1722,-438.5 1633,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-452.8\">dense: Dense</text>\n",
       "</g>\n",
       "<!-- 139793401368928&#45;&gt;139793401372176 -->\n",
       "<g class=\"edge\" id=\"edge30\">\n",
       "<title>139793401368928-&gt;139793401372176</title>\n",
       "<path d=\"M1677.5,-511.4551C1677.5,-503.3828 1677.5,-493.6764 1677.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-484.5903 1677.5,-474.5904 1674.0001,-484.5904 1681.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793401369264 -->\n",
       "<g class=\"node\" id=\"node30\">\n",
       "<title>139793401369264</title>\n",
       "<polygon fill=\"none\" points=\"1554.5,-365.5 1554.5,-401.5 1800.5,-401.5 1800.5,-365.5 1554.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-379.8\">batch_normalization: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139793401372176&#45;&gt;139793401369264 -->\n",
       "<g class=\"edge\" id=\"edge31\">\n",
       "<title>139793401372176-&gt;139793401369264</title>\n",
       "<path d=\"M1677.5,-438.4551C1677.5,-430.3828 1677.5,-420.6764 1677.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-411.5903 1677.5,-401.5904 1674.0001,-411.5904 1681.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139793470178920 -->\n",
       "<g class=\"node\" id=\"node31\">\n",
       "<title>139793470178920</title>\n",
       "<polygon fill=\"none\" points=\"1622,-292.5 1622,-328.5 1733,-328.5 1733,-292.5 1622,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-306.8\">dropout: Dropout</text>\n",
       "</g>\n",
       "<!-- 139793401369264&#45;&gt;139793470178920 -->\n",
       "<g class=\"edge\" id=\"edge32\">\n",
       "<title>139793401369264-&gt;139793470178920</title>\n",
       "<path d=\"M1677.5,-365.4551C1677.5,-357.3828 1677.5,-347.6764 1677.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-338.5903 1677.5,-328.5904 1674.0001,-338.5904 1681.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792402156624 -->\n",
       "<g class=\"node\" id=\"node32\">\n",
       "<title>139792402156624</title>\n",
       "<polygon fill=\"none\" points=\"1626.5,-219.5 1626.5,-255.5 1728.5,-255.5 1728.5,-219.5 1626.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-233.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139793470178920&#45;&gt;139792402156624 -->\n",
       "<g class=\"edge\" id=\"edge33\">\n",
       "<title>139793470178920-&gt;139792402156624</title>\n",
       "<path d=\"M1677.5,-292.4551C1677.5,-284.3828 1677.5,-274.6764 1677.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-265.5903 1677.5,-255.5904 1674.0001,-265.5904 1681.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792402155560 -->\n",
       "<g class=\"node\" id=\"node33\">\n",
       "<title>139792402155560</title>\n",
       "<polygon fill=\"none\" points=\"1547.5,-146.5 1547.5,-182.5 1807.5,-182.5 1807.5,-146.5 1547.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-160.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139792402156624&#45;&gt;139792402155560 -->\n",
       "<g class=\"edge\" id=\"edge34\">\n",
       "<title>139792402156624-&gt;139792402155560</title>\n",
       "<path d=\"M1677.5,-219.4551C1677.5,-211.3828 1677.5,-201.6764 1677.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-192.5903 1677.5,-182.5904 1674.0001,-192.5904 1681.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792401895264 -->\n",
       "<g class=\"node\" id=\"node34\">\n",
       "<title>139792401895264</title>\n",
       "<polygon fill=\"none\" points=\"1615,-73.5 1615,-109.5 1740,-109.5 1740,-73.5 1615,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 139792402155560&#45;&gt;139792401895264 -->\n",
       "<g class=\"edge\" id=\"edge35\">\n",
       "<title>139792402155560-&gt;139792401895264</title>\n",
       "<path d=\"M1677.5,-146.4551C1677.5,-138.3828 1677.5,-128.6764 1677.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-119.5903 1677.5,-109.5904 1674.0001,-119.5904 1681.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139792402009952 -->\n",
       "<g class=\"node\" id=\"node35\">\n",
       "<title>139792402009952</title>\n",
       "<polygon fill=\"none\" points=\"1631.5,-.5 1631.5,-36.5 1723.5,-36.5 1723.5,-.5 1631.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1677.5\" y=\"-14.8\">output: Dense</text>\n",
       "</g>\n",
       "<!-- 139792401895264&#45;&gt;139792402009952 -->\n",
       "<g class=\"edge\" id=\"edge36\">\n",
       "<title>139792401895264-&gt;139792402009952</title>\n",
       "<path d=\"M1677.5,-73.4551C1677.5,-65.3828 1677.5,-55.6764 1677.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1681.0001,-46.5903 1677.5,-36.5904 1674.0001,-46.5904 1681.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(tf.keras.utils.model_to_dot(model, dpi=60).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "Para entrenar el modelo es igual al caso anterior, ya generados el conjunto de datos correspondiente. Lo entrenamos con ayuda de `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 [==============================] - 12s 173ms/step - loss: 2.2746 - accuracy: 0.2275\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 9s 138ms/step - loss: 1.8884 - accuracy: 0.2983\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 10s 153ms/step - loss: 1.7669 - accuracy: 0.3363\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 1.6714 - accuracy: 0.3666\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 8s 122ms/step - loss: 1.5923 - accuracy: 0.3969\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 8s 125ms/step - loss: 1.5395 - accuracy: 0.4100\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 8s 124ms/step - loss: 1.4795 - accuracy: 0.4465\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 9s 133ms/step - loss: 1.4296 - accuracy: 0.4660\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.3438 - accuracy: 0.5096\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 1.2874 - accuracy: 0.5285\n",
      "\n",
      "*** Validation loss: 1.6187872956780827 - accuracy: 0.40009447932243347\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('awesome_advanced_approach')\n",
    "\n",
    "with mlflow.start_run(nested=True):\n",
    "    # Log model hiperparameters first\n",
    "    # mlflow.log_param('description_features_layer_size', DESCRIPTION_FEATURES_LAYER_SIZE)\n",
    "    mlflow.log_param('description_filter_widths', FILTER_WIDTHS)\n",
    "    mlflow.log_param('description_filter_count', FILTER_COUNT)\n",
    "    mlflow.log_param('hidden_layer_size', HIDDEN_LAYER_SIZE)\n",
    "    mlflow.log_param('embedded_columns', embedded_columns)\n",
    "    mlflow.log_param('one_hot_columns', one_hot_columns)\n",
    "    mlflow.log_param('numerical_columns', numeric_columns)\n",
    "    \n",
    "    # Train\n",
    "    epochs = 10\n",
    "    history = model.fit(train_dataset, epochs=epochs)\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, accuracy = model.evaluate(dev_dataset, verbose=0)\n",
    "    print(\"\\n*** Validation loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "    mlflow.log_metric('epochs', epochs)\n",
    "    mlflow.log_metric('train_loss', history.history[\"loss\"][-1])\n",
    "    mlflow.log_metric('train_accuracy', history.history[\"accuracy\"][-1])\n",
    "    mlflow.log_metric('validation_loss', loss)\n",
    "    mlflow.log_metric('validation_accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando el modelo sobre los datos de evaluación para la competencia\n",
    "\n",
    "Una vez que tenemos definido nuestro modelo, el último paso es ponerlo a prueba en los datos de evaluación para generar un archivo para enviar a la competencia Kaggle.\n",
    "\n",
    "Comenzamos cargando el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>Description</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41401</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>Siu Pak just give birth on 13/6/10 to 6puppies...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>Very manja and gentle stray cat found, we woul...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>41326</td>\n",
       "      <td>Kali is a super playful kitten who is on the g...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    1     265       0       1       1       2       0             2   \n",
       "1     1    1     307       0       1       2       7       0             2   \n",
       "2     1    0     307       0       2       1       2       7             2   \n",
       "3     2   12     265       0       2       1       7       0             2   \n",
       "4     2    3     264       0       2       1       2       5             3   \n",
       "\n",
       "   FurLength  Vaccinated  Dewormed  Sterilized  Health  Quantity  Fee  State  \\\n",
       "0          2           3         3           3       1         1    0  41401   \n",
       "1          2           1         1           2       1         1    0  41326   \n",
       "2          1           2         2           2       1         6    0  41326   \n",
       "3          2           3         3           3       1         1    0  41326   \n",
       "4          3           1         1           2       1         1   50  41326   \n",
       "\n",
       "                                         Description  PID  \n",
       "0  I just found it alone yesterday near my apartm...    1  \n",
       "1  Their pregnant mother was dumped by her irresp...    2  \n",
       "2  Siu Pak just give birth on 13/6/10 to 6puppies...    7  \n",
       "3  Very manja and gentle stray cat found, we woul...    9  \n",
       "4  Kali is a super playful kitten who is on the g...   11  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'test.csv'))\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el conjunto de datos para darle al modelo entrenado\n",
    "\n",
    "Tenemos que preprocesar los datos de evaluación de la misma manera que preprocesamos los de entrenamiento (para que sean compatibles con lo esperado por el modelo). Por suerte, es tan simple como hacer un par de modificaciones a lo ya hecho previamente. Lo único que tenemos que tener en cuenta es que ahora el conjunto de datos no generará una etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': <tf.Tensor: id=6732, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=6733, shape=(1,), dtype=int32, numpy=array([265], dtype=int32)>,\n",
      " 'Breed2': <tf.Tensor: id=6734, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Color1': <tf.Tensor: id=6735, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'Color2': <tf.Tensor: id=6736, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n",
      " 'Color3': <tf.Tensor: id=6737, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=6738, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=6739, shape=(13,), dtype=int32, numpy=\n",
      "array([ 116,  429, 1371,  991,  189,    1, 7873, 1043,   62,  600,  728,\n",
      "          5,    1], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=6740, shape=(24,), dtype=float32, numpy=\n",
      "array([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 1., 1., 0., 0.], dtype=float32)>}\n",
      "\n",
      "{'Age': <tf.Tensor: id=6741, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=6742, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
      " 'Breed2': <tf.Tensor: id=6743, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Color1': <tf.Tensor: id=6744, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n",
      " 'Color2': <tf.Tensor: id=6745, shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>,\n",
      " 'Color3': <tf.Tensor: id=6746, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=6747, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=6748, shape=(47,), dtype=int32, numpy=\n",
      "array([ 945,  154,  256, 2049,  105,  403,  991, 4678,  552,  545,    1,\n",
      "        142,  134,  403,    1,  118,  210,   73,    1,  533,  387,   35,\n",
      "        394,  272,   98,   62,    1,  464,  411,  151, 1401,   42,  253,\n",
      "          1,  825,   35, 4660,  247, 4156, 1402, 1403,    1,   43,   52,\n",
      "        599,   38,    1], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=6749, shape=(24,), dtype=float32, numpy=\n",
      "array([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0.], dtype=float32)>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First tokenize the description\n",
    "\n",
    "test_dataset[\"TokenizedDescription\"] = test_dataset[\"Description\"]\\\n",
    "    .fillna(value=\"\").apply(tokenize_description)\n",
    "\n",
    "# Generate the basic TF dataset\n",
    "\n",
    "tf_test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_generator(test_dataset, True),\n",
    "    output_types=instance_types  # It should have the same instance types\n",
    ")\n",
    "\n",
    "for data in tf_test_dataset.take(2):  # The dataset only returns a data instance now (no target)\n",
    "    pprint(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding batches\n",
    "\n",
    "Por último, y previo a probar el modelo sobre los datos de evaluación, generamos el conjunto de datos \"rellenado\". \n",
    "\n",
    "A diferencia de los datos de entrenamiento y validación, en este caso no permutamos las instancias, pues necesitamos saber a que `PID` pertenece cada una.\n",
    "\n",
    "Por otra parte, utilizamos los mismos valores de `padding_shapes` y `padding_values` para el primer componente (el de los datos), ignorando el valor del segundo componente (el de las etiquetas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf_test_dataset.padded_batch(\n",
    "    BATCH_SIZE, \n",
    "    padded_shapes=padding_shapes[0], \n",
    "    padding_values=padding_values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correr el modelo\n",
    "\n",
    "El último paso es correr el modelo sobre los datos de evaluación para conseguir las predicciones a enviar a la competencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[\"AdoptionSpeed\"] = model.predict(test_data).argmax(axis=1)\n",
    "test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informe\n",
    "\n",
    "En principio, este segundo trabajo práctico tenía el mismo objetivo que el anterior: Predecir la velocidad de adopción de las mascotas. Sin embargo, la consigna de este trabajo requería la utilización de redes en escalera ya que era necesario procesar la descripción de cada una de las mascotas.\n",
    "\n",
    "Para poder procesar el texto usando redes neuronales es necesario realizar un pre-procesamiento de los textos.\n",
    "Para esto, lo primero que debemos realizar es:\n",
    "* Tokenizar las descripciones\n",
    "* Llevar todas las descripciones a un tamaño uniforme (en este práctico tomamos 55 palabras ya que el 75% de los datos tenían 55 palabras o menos en sus descripciones)\n",
    "* Generar word embeddings a partir del vocabulario presente en nuestras descripciones tokenizadas\n",
    "* Generar una matriz de embeddings de palabras a partir de los embeddings generados en el paso anterior\n",
    "\n",
    "El resto de los datos, serán tratados de manera similar a como fue en el práctico anterior. Primero se separan de acuerdo al tipo de dato en one-hot-encoding, embedded y numeric, que luego serán utilizados para crear el resto de las capas de entrada del modelo.\n",
    "\n",
    "Una vez que tenemos nuestra matriz de embeddings, podemos definir los inputs de nuestro modelo. Esta matriz de embeddings será utilizada para inicializar una capa de embeddings de nuestro modelo.\n",
    "\n",
    "El trabajo práctico sugería utilizar redes convolucionales o recurrentes. En este caso, optamos por utilizar una red convolucional para generar el feature map de las secuencias de texto.\n",
    "\n",
    "Una vez realizado esto, se realiza el feature map completo del modelo, se crean las capas del modelo y se compila.\n",
    "\n",
    "Por último, el modelo es entrenado con los datos de entrenamiento. Una vez que nuestro modelo está entrenado, podemos utilizarlo para generar predicciones para nuestro conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
